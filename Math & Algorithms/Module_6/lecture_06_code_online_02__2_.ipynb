{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2369EvqoDAR9"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpbZzAAZDASA",
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности реализаций градиентного бустинга xgboost, lightgbm и catboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KOy94-ZDASD"
   },
   "source": [
    "Мы будем использовать датасет \"Bank Marketing\". В датасете \"Bank Marketing\" обычно содержатся данные, связанные с маркетинговыми кампаниями банка. Вот некоторые общие колонки, которые могут присутствовать в датасете \"Bank Marketing\":\n",
    "\n",
    "1. **age**: Возраст клиента.\n",
    "\n",
    "2. **job**: Тип работы клиента.\n",
    "\n",
    "3. **marital**: Семейное положение клиента.\n",
    "\n",
    "4. **education**: Образование клиента.\n",
    "\n",
    "5. **default**: Есть ли у клиента кредитный дефолт (задолженность).\n",
    "\n",
    "6. **balance**: Баланс на счете клиента.\n",
    "\n",
    "7. **housing**: Имеет ли клиент ипотеку (жилую недвижимость).\n",
    "\n",
    "8. **loan**: Имеет ли клиент личный заем.\n",
    "\n",
    "9. **contact**: Способ связи с клиентом.\n",
    "\n",
    "10. **day**: День месяца, когда был выполнен последний контакт.\n",
    "\n",
    "11. **month**: Месяц года, когда был выполнен последний контакт.\n",
    "\n",
    "12. **duration**: Продолжительность последнего контакта в секундах.\n",
    "\n",
    "13. **campaign**: Количество контактов, выполненных во время данной кампании.\n",
    "\n",
    "14. **pdays**: Количество дней, прошедших с момента последнего контакта до предыдущей кампании.\n",
    "\n",
    "15. **previous**: Количество контактов, выполненных перед текущей кампанией.\n",
    "\n",
    "16. **poutcome**: Результат предыдущей маркетинговой кампании.\n",
    "\n",
    "17. **target_variable**: Целевая переменная, которую необходимо предсказать. Например, это может быть информация о том, стал ли клиент подписчиком (1) или нет (0) на определенное предложение банка.\n",
    "\n",
    "Обратите внимание, что фактические названия колонок и их типы данных могут различаться в зависимости от конкретного датасета \"Bank Marketing\", поэтому рекомендуется проверить описание колонок в вашем конкретном наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5TI7cZVDMuN",
    "outputId": "15894ae4-f0af-4ad2-f1b7-b4384dd7e841"
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn xgboost lightgbm catboost hyperopt optuna bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6Eg2YM-DASE"
   },
   "outputs": [],
   "source": [
    "# Библиотеки общего назначения\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Модули метрик\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Модули препроцессинга данных\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Библиотеки градиентного бустинга\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Загрузка данных из CSV файла\n",
    "url = 'https://datahub.io/machine-learning/bank-marketing/r/bank-marketing.csv'\n",
    "column_names = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
    "                'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous',\n",
    "                'poutcome', 'target_variable']\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data.columns = column_names\n",
    "data.head()\n",
    "\n",
    "# Разделение на признаки (X) и целевую переменную (y)\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable'].apply(lambda x: x - 1)\n",
    "\n",
    "\n",
    "# Разделение на обучающий и тестовый наборы данных\n",
    "'''Что тут нужно написать?''' = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Определение препроцессора для категориальных и числовых признаков\n",
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Объединение препроцессоров\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Создание пайплайна для препроцессора\n",
    "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Применение препроцессора к данным\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQUyMMjxDASF"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLLW5M1jDASF"
   },
   "source": [
    "XGBoost (eXtreme Gradient Boosting) - это библиотека градиентного бустинга, предназначенная для решения задач классификации и регрессии. Она основана на идее градиентного бустинга, который строит ансамбль слабых моделей (обычно деревьев решений) и объединяет их для получения более сильной и устойчивой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUjEzU1FDASG"
   },
   "source": [
    "Описание алгоритма XGBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - Каждое дерево строится поэтапно. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - Для построения дерева используется критерий информативности, такой как критерий Джини или энтропийный критерий, для определения наилучшего разбиения на каждом узле дерева.\n",
    "   - Деревья строятся с ограничением на их глубину или другими параметрами, чтобы избежать переобучения.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева. Веса определяются скоростью обучения (learning rate), которая контролирует влияние каждого дерева на итоговое предсказание модели.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - Дополнительные механизмы регуляризации в XGBoost помогают предотвратить переобучение и улучшить обобщающую способность модели.\n",
    "   - XGBoost предлагает несколько методов регуляризации, таких как L1- и L2-регуляризация (также известные как регуляризация Лассо и ридж), которые помогают контролировать сложность модели и предотвращать переобучение. Эти методы добавляют штрафы к функции потерь, которые зависят от весов модели.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - XGBoost использует адаптивную функцию потерь, которая сочетает в себе различные функции потерь в зависимости от значения целевой переменной. Например, для задачи классификации с двумя классами может использоваться кросс-энтропия ($L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$), а для задачи регрессии - среднеквадратичная ошибка $(\\frac{1}{n}\\sum (y - \\hat{y})^2)$.\n",
    "   \n",
    "7. Ансамблирование деревьев:\n",
    "   - Поскольку XGBoost строит ансамбль из нескольких деревьев, предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели. Модель объединяет прогнозы всех деревьев с учетом их весов, определенных на основе ошибок и значимости.\n",
    "\n",
    "8. Прогнозирование:\n",
    "    - После обучения модели XGBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HZwAApPDASG"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **colsample_bylevel**: Доля признаков, используемых для построения каждого дерева на каждом уровне. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "7. **colsample_bynode**: Доля признаков, используемых для построения каждого дерева в каждом узле разбиения. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "8. **gamma**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение gamma помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "9. **reg_alpha**: Параметр L1 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "10. **reg_lambda**: Параметр L2 регуляризации. Он помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8_sZQSDDASH"
   },
   "outputs": [],
   "source": [
    "# Библиотеки и модули оптимизации\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определение модели XGBoost\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [30]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "'''Как обучить grid_search по X_train и y_train?'''\n",
    "xgb_gridsearch_execution_time = '''Как можно определить время выполнения?'''\n",
    "print(\"Время работы xgb_gridsearch_execution_time:\", xgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_gridsearch_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWSQWzqEDASI"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "xgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_hyperopt_execution_time:\", xgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "# Лучшие параметры и значение метрики\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = '''Как сделать предсказание для model по X_test?'''\n",
    "xgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_hyperopt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyXPDbY9DASI"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "xgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_optuna_execution_time:\", xgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "xgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", xgb_optuna_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz1EU4dlDASJ"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "xgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы xgb_bayes_opt_execution_time:\", xgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "xgb_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"F1-мера на тестовом наборе данных:\", xgb_bayes_opt_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeM4ZGD_DASJ"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCCshNyWDASK"
   },
   "source": [
    "Описание алгоритма LightGBM:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - LightGBM использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - LightGBM использует оптимизированную версию алгоритма градиентного бустинга, которая основана на методе обучения по гистограммам. Это позволяет существенно ускорить процесс построения деревьев.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - LightGBM также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - LightGBM поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - LightGBM также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются для получения итогового предсказания модели.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели LightGBM можно использовать для прогнозирования на новых данных, аналогично XGBoost. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ps-1fi5DASK"
   },
   "source": [
    "При оптимизации модели XGBoost обычно оптимизируют следующие основные параметры:\n",
    "\n",
    "1. **max_depth**: Глубина дерева. Этот параметр контролирует, насколько глубоким может быть каждое дерево в ансамбле. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр определяет вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **n_estimators**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bytree**: Доля признаков, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку признаков для каждого дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **min_split_gain**: Минимальное уменьшение функции потерь, необходимое для выполнения расщепления в дереве. Этот параметр контролирует регуляризацию модели. Большее значение min_split_gain помогает предотвратить переобучение за счет увеличения порога для выполнения расщепления.\n",
    "\n",
    "7. **reg_alpha** и **reg_lambda**: Параметры L1 и L2 регуляризации. Они помогают контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S_6T_wDDASK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели LightGBM\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.001],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "lgb_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_gridsearch_execution_time:\", lgb_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE6TnKb4DASK"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'n_estimators': hp.choice('n_estimators', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "lgb_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_hyperopt_execution_time:\", lgb_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'max_depth': [3, 4, 5][best['max_depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'n_estimators': [30, 50, 100][best['n_estimators']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiSaVZ4FDASL"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [30, 50, 100])\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "lgb_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_optuna_execution_time:\", lgb_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "lgb_optuna_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_HST1STDASL"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "lgb_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы lgb_bayes_opt_execution_time:\", lgb_bayes_opt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных с лучшими параметрами\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = lgb.LGBMClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "lgb_bayes_opt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", lgb_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2ffJUp-DASL"
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4nLNa6oDASL"
   },
   "source": [
    "Описание алгоритма CatBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель CatBoost с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - CatBoost использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost и LightGBM. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - CatBoost применяет особый подход к кодированию категориальных признаков, называемый симметричным бинарным кодированием, который учитывает взаимодействия между категориями и признаками.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost и LightGBM.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - CatBoost также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost и LightGBM.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - CatBoost поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost и LightGBM.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - CatBoost также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели.\n",
    "   - CatBoost также учитывает веса деревьев на основе их ошибок и значимости, аналогично XGBoost и LightGBM.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели CatBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRmwOfZJDASM"
   },
   "source": [
    "При оптимизации модели CatBoost можно обратить внимание на следующие важные параметры:\n",
    "\n",
    "1. **depth**: Глубина дерева. Этот параметр определяет, насколько глубоким может быть каждое дерево. Большая глубина может привести к переобучению, поэтому его следует настраивать осторожно.\n",
    "\n",
    "2. **learning_rate**: Скорость обучения. Этот параметр контролирует вклад каждого дерева в ансамбле. Более низкое значение learning_rate требует большего числа деревьев для достижения хорошей производительности, но может улучшить обобщающую способность модели.\n",
    "\n",
    "3. **iterations**: Количество деревьев в ансамбле. Этот параметр указывает, сколько деревьев следует построить. Большее количество деревьев может улучшить производительность модели, но слишком большое число может привести к переобучению.\n",
    "\n",
    "4. **subsample**: Доля обучающих примеров, используемых для построения каждого дерева. Этот параметр контролирует случайную подвыборку данных для каждого дерева. Значение меньше 1.0 позволяет уменьшить переобучение и повысить устойчивость модели.\n",
    "\n",
    "5. **colsample_bylevel**: Доля признаков, используемых для построения каждого уровня дерева. Этот параметр контролирует случайную подвыборку признаков для каждого уровня дерева. Значение меньше 1.0 помогает уменьшить переобучение и повысить разнообразие деревьев.\n",
    "\n",
    "6. **l2_leaf_reg**: L2 регуляризация. Этот параметр помогает контролировать сложность модели и предотвращать переобучение путем добавления штрафов за большие значения весов.\n",
    "\n",
    "7. **random_strength**: Сила случайности для ускорения обучения. Этот параметр контролирует уровень случайности в выборе признаков при построении дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "8. **bagging_temperature**: Температура бэггинга. Этот параметр контролирует степень случайности в выборе объектов для обучения каждого дерева. Значение больше 0 добавляет случайность, что может помочь предотвратить переобучение.\n",
    "\n",
    "Важно отметить, что конкретные параметры и их значения могут варьироваться в зависимости от конкретной задачи и набора данных. Рекомендуется провести эксперименты и настраивать параметры с использованием кросс-валидации для достижения наилучшей производительности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARxiWMrZDASM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение модели CatBoost\n",
    "model = CatBoostClassifier(silent=True)\n",
    "\n",
    "# Определение сетки параметров для оптимизации\n",
    "# param_grid = {\n",
    "#     'depth': [3, 4, 5],\n",
    "#     'learning_rate': [0.1, 0.05, 0.01],\n",
    "#     'n_estimators': [30, 50, 100]\n",
    "# }\n",
    "param_grid = {\n",
    "    'depth': [5],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [30, 50, 100]\n",
    "}\n",
    "\n",
    "# Создание объекта GridSearchCV для оптимизации параметров\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "catboost_gridsearch_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_gridsearch_execution_time:\", catboost_gridsearch_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "print(\"Лучшая F1:\", grid_search.best_score_)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **grid_search.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_gridsearch_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_gridsearch_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kV4FOtyDDASM"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(params):\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -f1_score(y_test, y_pred)  # Целевая метрика, отрицательная F1-мера\n",
    "\n",
    "# Определение пространства поиска параметров\n",
    "space = {\n",
    "    'depth': hp.choice('depth', [3, 4, 5]),\n",
    "    'learning_rate': hp.choice('learning_rate', [0.1, 0.01, 0.001]),\n",
    "    'iterations': hp.choice('iterations', [30, 50, 100])\n",
    "}\n",
    "\n",
    "# Выполнение оптимизации\n",
    "trials = Trials()\n",
    "start_time = time.time()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "catboost_hyperopt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_hyperopt_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = {\n",
    "    'depth': [3, 4, 5][best['depth']],\n",
    "    'learning_rate': [0.1, 0.01, 0.001][best['learning_rate']],\n",
    "    'iterations': [30, 50, 100][best['iterations']]\n",
    "}\n",
    "best_score = -trials.best_trial['result']['loss']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "catboost_hyperopt_f1 = f1_score(y_pred, y_test)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_hyperopt_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlBubWUyDASM"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'iterations': trial.suggest_categorical('iterations', [30, 50, 100])\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1  # Целевая метрика, максимизируем F1-меру\n",
    "\n",
    "# Создание объекта Study для оптимизации\n",
    "study = optuna.create_study(direction='maximize')\n",
    "start_time = time.time()\n",
    "study.optimize(objective, n_trials=10)\n",
    "catboost_optuna_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_hyperopt_execution_time:\", catboost_optuna_execution_time)\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_score)\n",
    "\n",
    "# Оценка качества модели на тестовом наборе данных\n",
    "model = CatBoostClassifier(silent=True, **best_params)\n",
    "model.fit(X_train, y_train)\n",
    "catboost_optuna_f1 = f1_score(y_test, model.predict(X_test))\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_optuna_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZ9EjWqWDASN"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Определение функции для оптимизации\n",
    "def objective(max_depth, learning_rate, n_estimators):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    model = CatBoostClassifier(silent=True, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Определение диапазона значений параметров для оптимизации\n",
    "pbounds = {\n",
    "    'max_depth': (3, 5),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'n_estimators': (30, 100)\n",
    "}\n",
    "\n",
    "# Создание объекта BayesianOptimization для оптимизации\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=2, n_iter=10)\n",
    "catboost_bayes_opt_execution_time = time.time() - start_time\n",
    "print(\"Время работы catboost_bayes_opt_execution_time:\", catboost_bayes_opt_execution_time)\n",
    "\n",
    "\n",
    "# Лучшие параметры и значение метрики\n",
    "best_params = optimizer.max['params']\n",
    "best_f1 = optimizer.max['target']\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)\n",
    "print(\"Лучшая F1-мера:\", best_f1)\n",
    "\n",
    "# Оценка качества модели с лучшими параметрами на тестовом наборе данных\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_model = CatBoostClassifier(silent=True, **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "catboost_bayes_opt_f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 на тестовом наборе данных:\", catboost_bayes_opt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3cyFO6cDASN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создание данных для таблицы сравнения по f1_score и времени работы\n",
    "params_match = [\n",
    "    ['XGBoost gridsearch', f'{xgb_gridsearch_f1}', f'{xgb_gridsearch_execution_time}'],\n",
    "    ['XGBoost hyperopt', f'{xgb_hyperopt_f1}', f'{xgb_hyperopt_execution_time}'],\n",
    "    ['XGBoost optuna', f'{xgb_optuna_f1}', f'{xgb_optuna_execution_time}'],\n",
    "    ['XGBoost bayes_opt', f'{xgb_bayes_opt_f1}', f'{xgb_bayes_opt_execution_time}'],\n",
    "    ['LightGBM gridsearch', f'{lgb_gridsearch_f1}', f'{lgb_gridsearch_execution_time}'],\n",
    "    ['LightGBM hyperopt', f'{lgb_hyperopt_f1}', f'{lgb_hyperopt_execution_time}'],\n",
    "    ['LightGBM optuna', f'{lgb_optuna_f1}', f'{lgb_optuna_execution_time}'],\n",
    "    ['LightGBM bayes_opt', f'{lgb_bayes_opt_f1}', f'{lgb_bayes_opt_execution_time}'],\n",
    "    ['Catboost gridsearch', f'{catboost_gridsearch_f1}', f'{catboost_gridsearch_execution_time}'],\n",
    "    ['Catboost hyperopt', f'{catboost_hyperopt_f1}', f'{catboost_hyperopt_execution_time}'],\n",
    "    ['Catboost optuna', f'{catboost_optuna_f1}', f'{catboost_optuna_execution_time}'],\n",
    "    ['Catboost bayes_opt', f'{catboost_bayes_opt_f1}', f'{catboost_bayes_opt_execution_time}'],\n",
    "\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Модель и оптимайзер', 'F1', 'Время'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df.head(len(params_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxuCOJzkDASN"
   },
   "outputs": [],
   "source": [
    "# Создание данных для таблицы\n",
    "params_match = [\n",
    "    ['Скорость обучения (Learning Rate)', 'eta (learning_rate)', 'learning_rate', 'learning_rate'],\n",
    "    ['Количество деревьев (Number of Trees)', 'n_estimators', 'n_estimators', 'iterations'],\n",
    "    ['Максимальная глубина дерева (Maximum Depth)', 'max_depth', 'max_depth', 'max_depth'],\n",
    "    ['Минимальный вес листа дерева, Минимальное количество объектов в листе (Minimum Child Weight)', 'min_child_weight', 'min_child_samples', 'min_data_in_leaf'],\n",
    "    ['Гамма (Gamma)', 'gamma', 'min_split_gain', 'l2_leaf_reg'],\n",
    "    ['Доля выборки объектов (Subsample Ratio)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Доля выборки признаков (Column Subsampling Ratio)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Лямбда (L2-регуляризация)', 'reg_lambda', 'reg_lambda', 'reg_lambda'],\n",
    "    ['Альфа (L1-регуляризация)', 'reg_alpha', 'reg_alpha', 'reg_alpha'],\n",
    "    ['Максимальное изменение шага (Maximum Delta Step)', 'max_delta_step', '-', '-'],\n",
    "    ['Максимальное количество корзин (Maximum Bin Count)', 'max_bin', 'max_bin', 'max_bin'],\n",
    "    ['Частота сэмплирования объектов (Sampling Frequency for LGBM)', '-', 'subsample_freq', '-'],\n",
    "    ['Масштабирование веса положительного класса (Scale the Weight of Positive Class)', 'scale_pos_weight', 'scale_pos_weight', 'scale_pos_weight'],\n",
    "    ['Режим обучения с помощью градиентного спуска (Booster Type)', 'booster', 'boosting_type', 'boosting_type'],\n",
    "    ['Коэффициент сжатия (Subsample)', 'subsample', 'subsample', 'subsample'],\n",
    "    ['Размер выборки для построения дерева (Bagging Fraction)', 'colsample_bytree', 'colsample_bytree', 'colsample_bylevel'],\n",
    "    ['Способ выбора ветвления (Split Criterion)', 'tree_method', 'tree_learner', 'grow_policy'],\n",
    "    ['Функция потерь (Loss Function)', 'objective', 'objective', 'loss_function'],\n",
    "    ['Максимальное количество листьев (Max Leaf Nodes)', '-', 'num_leaves', 'max_leaves']\n",
    "]\n",
    "\n",
    "# Создание DataFrame из данных\n",
    "df = pd.DataFrame(params_match, columns=['Описание параметра', 'XGBoost', 'LightGBM', 'CatBoost'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Отображение DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVVpxP1-zxyW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
