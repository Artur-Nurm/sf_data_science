{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyQhsPJoxYMX"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZWG8AhxxYMZ",
    "tags": []
   },
   "source": [
    "# Цель занятия\n",
    "На этом занятии мы рассмотрим особенности моделей градиентного бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6s1omRZxYMb"
   },
   "source": [
    "## Базовое дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRZNeXFFxYMb"
   },
   "outputs": [],
   "source": [
    "class BasicTree:\n",
    "\n",
    "    def __init__(self, max_depth=None, criterion=\"entropy\"):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_split_entropy(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий энтропии.\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        def entropy(y):\n",
    "            \"\"\"\n",
    "            Вычисляет энтропию вектора y со значениями дискретных переменных.\n",
    "            Аргументы:\n",
    "            - y: вектор numpy с дискретными значениями.\n",
    "            Возвращает:\n",
    "            - entropy: значение энтропии типа float.\n",
    "            \"\"\"\n",
    "            # Подсчитываем количество каждого уникального значения в y.\n",
    "            _, counts = np.unique(y, return_counts=True)\n",
    "            # Вычисляем вероятность каждого уникального значения.\n",
    "            probs = counts / len(y)\n",
    "            # Вычисляем значение энтропии.\n",
    "            return '''Напишите формулу энтропии'''\n",
    "\n",
    "        best_feature, best_threshold, best_gain = None, None, 0\n",
    "        # Итерируемся по всем признакам.\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Находим уникальные значения признака.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Итерируемся по всем возможным пороговым значениям признака.\n",
    "            for threshold in thresholds:\n",
    "                # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                # Вычисляем значение критерия энтропии для текущего разбиения.\n",
    "                gain = '''Что здесь нужно написать?''' - (len(left_y) / len(y)) * entropy(left_y) \\\n",
    "                                       - (len(right_y) / len(y)) * entropy(right_y)\n",
    "                # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_split_gini(X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y, используя критерий Джини.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с дискретными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_gain: значение критерия энтропии для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        # Завершите реализацию функции gini_index\n",
    "\n",
    "        def gini_index(y):\n",
    "\n",
    "            \"\"\"\n",
    "            Вычисляет критерий Джини для вектора y со значениями дискретных переменных.\n",
    "\n",
    "            Аргументы:\n",
    "            - y: вектор numpy с дискретными значениями.\n",
    "\n",
    "            Возвращает:\n",
    "            - gini: значение критерия Джини типа float.\n",
    "\n",
    "            Подсчитываем количество каждого уникального значения в y.\n",
    "            \"\"\"\n",
    "            _, counts = np.unique(y, return_counts=True)\n",
    "            # Вычисляем вероятность каждого уникального значения.\n",
    "            probs = '''Как определить вероятность?'''\n",
    "\n",
    "            if not len(probs):\n",
    "                return 0\n",
    "\n",
    "            criterion  = 1 - np.sum(probs ** 2)\n",
    "            return criterion\n",
    "\n",
    "        best_feature, best_threshold, best_gain = None, None, 0\n",
    "        # Итерируемся по всем признакам.\n",
    "        for feature in range(X.shape[1]):\n",
    "            # Находим уникальные значения признака.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            # Итерируемся по всем возможным пороговым значениям признака.\n",
    "            for threshold in thresholds:\n",
    "                # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                # Пропускаем текущую итерацию, если не найдены объекты, которые относятся к левому или правому поддереву.\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                # Определяем вектор целевой переменной для объектов, которые относятся к левому и правому поддереву.\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "\n",
    "                gain = gini_index(y) - (len(left_y) / len(y)) * gini_index(left_y) - (len(right_y) / len(y)) * gini_index(right_y)\n",
    "                # Обновляем значения лучшего разбиения, если найдено разбиение с большим значением\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_split_mse(X, y):\n",
    "        \"\"\"\n",
    "        Находит лучшее разбиение для вектора признаков X и вектора целевой переменной y,\n",
    "        используя критерий среднеквадратичной ошибки (MSE).\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - best_feature: индекс признака, по которому было найдено лучшее разбиение.\n",
    "        - best_threshold: значение порога, по которому было найдено лучшее разбиение.\n",
    "        - best_mse: значение критерия среднеквадратичной ошибки для лучшего разбиения.\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_mse = None, None, float('inf')\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                mse = np.mean((left_y - np.mean(left_y))**2) + np.mean((right_y - np.mean(right_y))**2)\n",
    "                if mse < best_mse:\n",
    "                    best_feature, best_threshold, best_mse = feature, threshold, mse\n",
    "        return best_feature, best_threshold, best_mse\n",
    "\n",
    "    def fit(self, X, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Обучает дерево регрессии на обучающих данных X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        \"\"\"\n",
    "        if y_pred is not None:\n",
    "            self.tree = self._build_tree(X, y, depth=0, y_pred=y_pred)\n",
    "        else:\n",
    "            self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Выполняет предсказание для входных данных X.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: вектор numpy с предсказанными вещественными значениями.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth, y_pred=None):\n",
    "        \"\"\"\n",
    "        Рекурсивно строит дерево регрессии, используя входные данные X и y.\n",
    "\n",
    "        Аргументы:\n",
    "        - X: вектор numpy с вещественными значениями признаков.\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "        - y_pred (опционально): вектор numpy с вещественными значениями предсказаний (для Gradient boosting)\n",
    "        - depth: текущая глубина дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий узел дерева.\n",
    "\n",
    "        \"\"\"\n",
    "        # Проверка условий останова по максимальной глубине и другим критериям\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            # Создание листового узла\n",
    "            return self._create_leaf_node(y, y_pred)\n",
    "\n",
    "        # Нахождение лучшего разбиения по критерию энтропии\n",
    "        if self.criterion == \"entropy\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_entropy(X, y)\n",
    "\n",
    "        # Нахождение лучшего разбиения по критерию джини\n",
    "        elif self.criterion == \"gini\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_gini(X, y)\n",
    "\n",
    "        # Нахождение лучшего разбиения по критерию mse\n",
    "        elif self.criterion == \"mse\":\n",
    "            best_feature, best_threshold, _ = self.find_best_split_mse(X, y)\n",
    "\n",
    "        else:\n",
    "            raise Exception('Следует задать критерий разбиения из списка [\"mse\", \"entropy\"]')\n",
    "\n",
    "        # Проверка условия останова, если не удалось найти лучшее разбиение\n",
    "        if best_feature is None or best_threshold is None:\n",
    "            # Создание листового узла\n",
    "            if y_pred is not None:\n",
    "                return self._create_leaf_node(y, y_pred)\n",
    "            return self._create_leaf_node(y)\n",
    "\n",
    "        # Разделение данных на левое и правое поддеревья\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Рекурсивное построение левого и правого поддеревьев\n",
    "        if y_pred is not None:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1, y_pred[left_indices])\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1, y_pred[right_indices])\n",
    "        else:\n",
    "            left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "            right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        # Создание узла с информацией о лучшем разбиении\n",
    "        node = {'feature': best_feature, 'threshold': best_threshold,\n",
    "                'left': left_tree, 'right': right_tree}\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        raise NotImplementedError(\"Метод требует переопределения в классе наследования\")\n",
    "\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Обходит дерево регрессии для выполнения предсказания на входных данных x.\n",
    "\n",
    "        Аргументы:\n",
    "        - x: вектор numpy с вещественными значениями признаков.\n",
    "        - node: текущий узел дерева.\n",
    "\n",
    "        Возвращает:\n",
    "        - value: предсказанное вещественное значение.\n",
    "        \"\"\"\n",
    "        if 'value' in node:\n",
    "            return node['value']\n",
    "\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._traverse_tree(x, node['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node['right'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDA4gmr5xYMe"
   },
   "source": [
    "## Дерево для регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akdoT_5VxYMe"
   },
   "outputs": [],
   "source": [
    "class RegressionTree(BasicTree):\n",
    "    def __init__(self, max_depth=None, criterion=\"mse\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект RegressionTree.\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально).\n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка:\n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred=None):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева регрессии.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел среднего значения целевой переменной.\n",
    "        \"\"\"\n",
    "        return {'value': np.mean(y)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAqCRH4jxYMf"
   },
   "source": [
    "Входные данные: Data $\\{(x_i, y_i)\\}_{i=1}^n$\n",
    "\n",
    "Дифференцируемая функция потерь $L(y_i, F(x_i))$ (квадратичная функция потерь как наиболее часто используемая)\n",
    "\n",
    "Шаг 1: Инициализировать модель константным значением: $F_0(x)=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{i=1}^n L(y_i, \\gamma)$, где $\\gamma$ - предсказанное значение. Обычно используется среднее значение.\n",
    "\n",
    "Шаг 2: for $m=1$ to $M$ : (для каждого из M деревьев в ансамбле)\n",
    "\n",
    "(A) Вычисление остатков (residuals) $r_{i m}=-\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$ for $i=1, \\ldots, n$ (i - i-й обучающий пример)\n",
    "\n",
    "(B) Обучить дерево на объектах $r_{\\text {im }}$ и получить $J$ листьев $R_{j m}$, for $j=1 \\ldots J_m$\n",
    "\n",
    "(C) Для каждого листа $j=1 \\ldots J_m$ вычислить предсказание $\\gamma_{j m}=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{x_i \\in R_{i j}} L\\left(y_i, F_{m-1}\\left(x_i\\right)+\\gamma\\right)$\n",
    "\n",
    "(D) Добавить текущее предсказание к уже полученному значению на предыдущих итерациях $F_m(x)=F_{m-1}(x)+\\nu \\sum_{j=1}^{J_m} \\gamma_{j m} I\\left(x \\in R_{j m}\\right)$ , где $\\nu$ - скорость обучения.\n",
    "\n",
    "Шаг 3: Получить итоговые предсказания $F_M(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z__ihYdBxYMg"
   },
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Инициализируем предсказания базовой модели нулевым значением\n",
    "        predictions = np.full_like(y, 0)\n",
    "        # Инициализируем предсказания базовой модели средним значением\n",
    "#         predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Вычисляем остатки между предсказаниями и истинными значениями\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Создаем экземпляр регрессионного дерева\n",
    "            tree = RegressionTree(max_depth=self.max_depth)\n",
    "\n",
    "            # Обучаем дерево на остатках\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Получаем предсказания для текущего дерева\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Обновляем предсказания ансамбля\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "            # Сохраняем дерево в список базовых моделей\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Инициализируем предсказания нулевым значением\n",
    "        predictions = np.zeros(len(X))\n",
    "#         predictions = np.full(len(X), np.mean(y))\n",
    "\n",
    "        for tree in self.estimators:\n",
    "            # Получаем предсказания для каждого дерева в ансамбле\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Обновляем предсказания ансамбля\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFv7r60RxYMh",
    "outputId": "26667d60-121c-4ddb-af34-eaf6ca45e1c6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Генерация случайных данных для задачи регрессии\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение модели градиентного бустинга\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оценка качества модели с помощью среднеквадратичной ошибки\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.scatter(X_test, y_test, color='b', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='r', label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2pxiHUNxYMi"
   },
   "source": [
    "## Дерево для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLw_iT7kxYMj"
   },
   "source": [
    "Реализация по: https://www.youtube.com/watch?v=StWY5QWMXCw и\n",
    "https://maelfabien.github.io/machinelearning/GradientBoostC/#\n",
    "\n",
    "\"Log odds\" (логарифм шансов) относится к математической функции, которая измеряет отношение вероятности успеха к вероятности неудачи и берет ее логарифм. Она используется для преобразования вероятностей в непрерывный диапазон и представления их в линейном масштабе.\n",
    "\n",
    "Формально, лог odds отношения шансов вычисляется следующим образом:\n",
    "\n",
    "$\n",
    "\\text{log odds} = \\log\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    "где:\n",
    "- log odds - логарифм шансов (log odds),\n",
    "- p - вероятность успеха (или принадлежности к классу 1),\n",
    "- 1-p - вероятность неудачи (или принадлежности к классу 0).\n",
    "\n",
    "Лог odds представляет собой численное значение, которое может быть положительным или отрицательным, в зависимости от отношения вероятностей успеха и неудачи. Эта мера широко используется в статистике и машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqsRt4AKxYMj"
   },
   "source": [
    "Входные данные: Data $\\{(x_i, y_i)\\}_{i=1}^n$\n",
    "\n",
    "Дифференцируемая функция потерь $L(y_i, F(x_i))$\n",
    "\n",
    "Шаг 1: Инициализировать модель константным значением: $F_0(x)=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{i=1}^n L(y_i, \\gamma)$\n",
    "\n",
    "Шаг 2: for $m=1$ to $M$ : (для каждого из M деревьев в ансамбле)\n",
    "\n",
    "(A) Вычисление остатков (residuals) $r_{i m}=-\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$ for $i=1, \\ldots, n$ (i - i-й обучающий пример)\n",
    "\n",
    "(B) Обучить дерево на объектах $r_{\\text {im }}$ и получить $J$ листьев $R_{j m}$, for $j=1 \\ldots J_m$\n",
    "\n",
    "(C) Для каждого листа $j=1 \\ldots J_m$ вычислить предсказание $\\gamma_{j m}=\\underset{\\gamma}{\\operatorname{argmin}} \\sum_{x_i \\in R_{i j}} L\\left(y_i, F_{m-1}\\left(x_i\\right)+\\gamma\\right)$\n",
    "\n",
    "(D) Добавить текущее предсказание к уже полученному значению на предыдущих итерациях $F_m(x)=F_{m-1}(x)+\\nu \\sum_{j=1}^{J_m} \\gamma_{j m} I\\left(x \\in R_{j m}\\right)$ , где $\\nu$ - скорость обучения.\n",
    "\n",
    "Шаг 3: Получить итоговые предсказания $F_M(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DLcF8NXxYMk"
   },
   "outputs": [],
   "source": [
    "class GBClassificationRegressionTree(BasicTree):\n",
    "\n",
    "    def __init__(self, max_depth=None, criterion=\"entropy\"):\n",
    "        \"\"\"\n",
    "        Инициализирует объект GBClassificationRegressionTree.\n",
    "        Реализация по: https://www.youtube.com/watch?v=StWY5QWMXCw и\n",
    "        https://maelfabien.github.io/machinelearning/GradientBoostC/#\n",
    "\n",
    "        Аргументы:\n",
    "        - max_depth: максимальная глубина дерева (опционально).\n",
    "        - criterion: выбор способа разбиений деревьев. Выбирается из списка:\n",
    "        [\"mse\", \"entropy\", \"gini\"]\n",
    "        Если значение None, то дерево будет строиться без ограничения глубины.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def _create_leaf_node(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Создает листовой узел дерева регрессии.\n",
    "\n",
    "        Аргументы:\n",
    "        - y: вектор numpy с вещественными значениями целевой переменной.\n",
    "\n",
    "        Возвращает:\n",
    "        - node: словарь, представляющий листовой узел среднего значения целевой переменной.\n",
    "        \"\"\"\n",
    "        return {'value': np.sum(y) / np.sum(y_pred * (1 - y_pred))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peFIG52bxYMk"
   },
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=None):\n",
    "        \"\"\"\n",
    "        Инициализация класса GradientBoostingClassifier\n",
    "\n",
    "        Параметры:\n",
    "        - n_estimators: int, количество базовых моделей (деревьев решений)\n",
    "        - learning_rate: float, скорость обучения (шаг градиентного спуска)\n",
    "        - max_depth: int, максимальная глубина деревьев решений\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение модели GradientBoostingClassifier\n",
    "\n",
    "        Параметры:\n",
    "        - X: numpy.array, массив признаков\n",
    "        - y: numpy.array, массив целевых значений\n",
    "        \"\"\"\n",
    "        # Инициализация начальных предсказаний средним значением y\n",
    "        y_pred = np.full_like(y, self._sigmoid(np.log(np.count_nonzero(y == 1) / np.count_nonzero(y == 0))),\n",
    "                              dtype=np.float64)\n",
    "\n",
    "#         y_pred = np.full_like(y, 0, dtype=np.float64)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Вычисление остатков (residuals) как разницы между y и предсказаниями\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Создание и обучение базовой модели (дерева решений) на остатках\n",
    "            estimator = GBClassificationRegressionTree(max_depth=self.max_depth)\n",
    "            estimator.fit(X, residuals, y_pred)\n",
    "\n",
    "            # Добавление базовой модели в список\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "            # Обновление предсказаний путем добавления произведения скорости обучения и предсказаний базовой модели\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание классов для новых данных\n",
    "\n",
    "        Параметры:\n",
    "        - X: numpy.array, массив признаков для предсказания\n",
    "\n",
    "        Возвращает:\n",
    "        - predictions: numpy.array, массив предсказанных классов (0 или 1)\n",
    "        \"\"\"\n",
    "        # Инициализация предсказаний с константным значением 0.5\n",
    "        y_pred = np.full(len(X), 0.5)\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            # Обновление предсказаний путем добавления произведения скорости обучения и предсказаний базовой модели\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "\n",
    "        # Округление предсказаний и преобразование в целочисленный тип\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Функция сигмоиды\n",
    "\n",
    "        Параметры:\n",
    "        - x: numpy.array, массив значений\n",
    "\n",
    "        Возвращает:\n",
    "        - numpy.array, массив значений, преобразованных с помощью сигмоиды\n",
    "        \"\"\"\n",
    "        return '''Напишите формулу сигмоиды'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBm2QQrkxYMl"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Генерация синтетических данных\n",
    "X, y = make_classification(n_classes=2, n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание и обучение модели GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Прогнозирование классов для тестовой выборки\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Вычисление и вывод точности модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G_ARP-KxYMl"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjcacV03xYMm"
   },
   "source": [
    "XGBoost (eXtreme Gradient Boosting) - это библиотека градиентного бустинга, предназначенная для решения задач классификации и регрессии. Она основана на идее градиентного бустинга, который строит ансамбль слабых моделей (обычно деревьев решений) и объединяет их для получения более сильной и устойчивой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq_Fzm4zxYMm"
   },
   "source": [
    "Описание алгоритма XGBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - Каждое дерево строится поэтапно. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - Для построения дерева используется критерий информативности, такой как критерий Джини или энтропийный критерий, для определения наилучшего разбиения на каждом узле дерева.\n",
    "   - Деревья строятся с ограничением на их глубину или другими параметрами, чтобы избежать переобучения.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева. Веса определяются скоростью обучения (learning rate), которая контролирует влияние каждого дерева на итоговое предсказание модели.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - Дополнительные механизмы регуляризации в XGBoost помогают предотвратить переобучение и улучшить обобщающую способность модели.\n",
    "   - XGBoost предлагает несколько методов регуляризации, таких как L1- и L2-регуляризация (также известные как регуляризация Лассо и ридж), которые помогают контролировать сложность модели и предотвращать переобучение. Эти методы добавляют штрафы к функции потерь, которые зависят от весов модели.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - XGBoost использует адаптивную функцию потерь, которая сочетает в себе различные функции потерь в зависимости от значения целевой переменной. Например, для задачи классификации с двумя классами может использоваться кросс-энтропия ($L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$), а для задачи регрессии - среднеквадратичная ошибка $(\\frac{1}{n}\\sum (y - \\hat{y})^2)$.\n",
    "   \n",
    "7. Ансамблирование деревьев:\n",
    "   - Поскольку XGBoost строит ансамбль из нескольких деревьев, предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели. Модель объединяет прогнозы всех деревьев с учетом их весов, определенных на основе ошибок и значимости.\n",
    "\n",
    "8. Прогнозирование:\n",
    "    - После обучения модели XGBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vPOTfH-xYMm"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUecA0etxYMn"
   },
   "source": [
    "Описание алгоритма LightGBM:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - LightGBM использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - LightGBM использует оптимизированную версию алгоритма градиентного бустинга, которая основана на методе обучения по гистограммам. Это позволяет существенно ускорить процесс построения деревьев.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - LightGBM также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - LightGBM поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - LightGBM также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются для получения итогового предсказания модели.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели LightGBM можно использовать для прогнозирования на новых данных, аналогично XGBoost. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpW4-sO8xYMn"
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkeoCZqtxYMn"
   },
   "source": [
    "Описание алгоритма CatBoost:\n",
    "\n",
    "1. Инициализация модели:\n",
    "   - Инициализируется модель CatBoost с нулевыми предсказаниями (например, средним значением целевой переменной для задачи регрессии или логарифмом отношения шансов для задачи классификации).\n",
    "\n",
    "2. Построение базовых моделей (деревьев решений):\n",
    "   - CatBoost использует алгоритм градиентного бустинга над решающими деревьями.\n",
    "   - Деревья строятся поэтапно, аналогично XGBoost и LightGBM. На каждом этапе добавляется новое дерево с учетом остатков, оставшихся после предыдущих деревьев.\n",
    "   - CatBoost применяет особый подход к кодированию категориальных признаков, называемый симметричным бинарным кодированием, который учитывает взаимодействия между категориями и признаками.\n",
    "\n",
    "3. Вычисление градиентов и обновление предсказаний:\n",
    "   - После построения каждого дерева вычисляются градиенты ошибки между предсказаниями модели и истинными значениями целевой переменной.\n",
    "   - Предсказания модели обновляются путем добавления взвешенной версии предсказаний нового дерева, аналогично XGBoost и LightGBM.\n",
    "\n",
    "4. Регуляризация и предотвращение переобучения:\n",
    "   - CatBoost также предлагает несколько методов регуляризации для предотвращения переобучения модели.\n",
    "   - Он поддерживает L1-регуляризацию (регуляризацию Лассо) и L2-регуляризацию (регуляризацию ридж), которые добавляют штрафы к функции потерь, аналогично XGBoost и LightGBM.\n",
    "\n",
    "6. Функция потерь:\n",
    "   - CatBoost поддерживает различные функции потерь, в зависимости от типа задачи (классификация или регрессия). Для задачи бинарной классификации часто используется логистическая функция потерь, а для задачи регрессии - среднеквадратичная ошибка (MSE), аналогично XGBoost и LightGBM.\n",
    "\n",
    "7. Ансамблирование деревьев:\n",
    "   - CatBoost также строит ансамбль из нескольких деревьев, и предсказания каждого дерева складываются, чтобы получить итоговое предсказание модели.\n",
    "   - CatBoost также учитывает веса деревьев на основе их ошибок и значимости, аналогично XGBoost и LightGBM.\n",
    "\n",
    "8. Прогнозирование:\n",
    "   - После обучения модели CatBoost можно использовать для прогнозирования на новых данных. Модель принимает входные признаки и возвращает прогнозы для задач классификации или регрессии."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
